{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "The STANDARD Data pre and Spell check (manual).ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ncpitrsb/The-STANDARD-Data-pre-and-Spell-checker-manual-/blob/main/The_STANDARD_Data_pre_and_Spell_check_(manual).ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XPSll_GhZ6tA",
        "outputId": "e65ec844-f592-4d5c-c634-c7eee8de5321"
      },
      "source": [
        "f6 = open(\"/content/gdrive/MyDrive/TOTAL/‘ทุกคนรู้ แฟนคลับรู้’ ByteDance บริษัทแม่ TikTok รายได้โต 2 เท่า กำไรพุ่ง 9.5 หมื่นล้านบาท  proofread pa/1_clean.txt\",'r')\n",
        "earth = f6.read()\n",
        "print(earth[2493:2506])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "ets Bloomberg\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hjJ11k6if4L1"
      },
      "source": [
        "import csv\n",
        "with open('link2.csv', 'r') as file:\n",
        "    reader = csv.reader(file)\n",
        "    count = 0\n",
        "    link_list = []\n",
        "    for row in reader:\n",
        "      if (len(row[1][35:79]) == 44):\n",
        "        link_list.append(row[1][35:79])\n",
        "    print(link_list)\n",
        "    print(len(link_list))\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "R7re1ccx2fdw",
        "outputId": "2643feae-1f3a-4eb0-c368-c898f73c28d2"
      },
      "source": [
        "from google.colab import drive\n",
        "import json\n",
        "drive.mount(\"/content/gdrive\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/gdrive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yw-rKOcjwakt",
        "outputId": "c35ae7dc-ccce-4a7a-f0dc-594461004bec"
      },
      "source": [
        "!pip install pythaispell"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\u001b[31mERROR: Could not find a version that satisfies the requirement pythaispell (from versions: none)\u001b[0m\n",
            "\u001b[31mERROR: No matching distribution found for pythaispell\u001b[0m\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 327
        },
        "id": "zvRFX_jDwIqH",
        "outputId": "e9bea3df-4eb9-42c0-e93a-137319297fa0"
      },
      "source": [
        "import pythaispell\n",
        "print(pythaispell.spell(\"สวัสดีนะคับผม\"))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ModuleNotFoundError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-7-7770f74470cf>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mpythaispell\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpythaispell\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mspell\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"สวัสดีนะคับผม\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'pythaispell'",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jzpCE11N1xn_"
      },
      "source": [
        "%%capture\n",
        "!pip install --upgrade --pre attacut"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d882HzTzwMkv",
        "outputId": "af179a61-58ce-42d2-fdce-405aa2f9407e"
      },
      "source": [
        "!pip install sklearn-crfsuite"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting sklearn-crfsuite\n",
            "  Downloading https://files.pythonhosted.org/packages/25/74/5b7befa513482e6dee1f3dd68171a6c9dfc14c0eaa00f885ffeba54fe9b0/sklearn_crfsuite-0.3.6-py2.py3-none-any.whl\n",
            "Requirement already satisfied: python-crfsuite>=0.8.3 in /usr/local/lib/python3.6/dist-packages (from sklearn-crfsuite) (0.9.7)\n",
            "Requirement already satisfied: tabulate in /usr/local/lib/python3.6/dist-packages (from sklearn-crfsuite) (0.8.7)\n",
            "Requirement already satisfied: tqdm>=2.0 in /usr/local/lib/python3.6/dist-packages (from sklearn-crfsuite) (4.41.1)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from sklearn-crfsuite) (1.15.0)\n",
            "Installing collected packages: sklearn-crfsuite\n",
            "Successfully installed sklearn-crfsuite-0.3.6\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kYRAziGkJAPl",
        "outputId": "52c36da8-d4d8-445d-d568-b8d341b25c13"
      },
      "source": [
        "!pip install pythainlp"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting pythainlp\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/c1/09/1215cb6f6ef0cfc9dbb427a961fda8a47c111955f782f659ca2d38c79adc/pythainlp-2.2.6-py3-none-any.whl (10.6MB)\n",
            "\u001b[K     |████████████████████████████████| 10.6MB 6.7MB/s \n",
            "\u001b[?25hRequirement already satisfied: python-crfsuite>=0.9.6 in /usr/local/lib/python3.6/dist-packages (from pythainlp) (0.9.7)\n",
            "Requirement already satisfied: requests>=2.22.0 in /usr/local/lib/python3.6/dist-packages (from pythainlp) (2.23.0)\n",
            "Collecting tinydb>=3.0\n",
            "  Downloading https://files.pythonhosted.org/packages/e5/7e/85ee672ee60affd5b4461efa19f260cf7575f36b94fbd1f0825742639910/tinydb-4.3.0-py3-none-any.whl\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests>=2.22.0->pythainlp) (2020.12.5)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests>=2.22.0->pythainlp) (2.10)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests>=2.22.0->pythainlp) (1.24.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests>=2.22.0->pythainlp) (3.0.4)\n",
            "Installing collected packages: tinydb, pythainlp\n",
            "Successfully installed pythainlp-2.2.6 tinydb-4.3.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3NfYBU_vizB2"
      },
      "source": [
        "Clean text loop"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r-O5xh7qsGOz"
      },
      "source": [
        "import os\n",
        " \n",
        "location = \"/content/gdrive/My Drive/TOTAL\"\n",
        "listFolderInPath = os.listdir(location) #list ของ Folder ที่อยู่ใน Location \n",
        "for i in listFolderInPath:\n",
        "  eachPathFolder = location + \"/\" + i\n",
        "  listOfFileInFolder = os.listdir(eachPathFolder) #list ของชื่อไฟล์ในโฟลเดอร์นั้น\n",
        "  count = 1\n",
        "  for j in listOfFileInFolder:\n",
        "    oldPathOfFileInfolder = \"/content/gdrive/My Drive/TOTAL/\"+i+\"/\"+j                   #ชื่อเดิม\n",
        "    newPathOfFileInfolder = \"/content/gdrive/My Drive/TOTAL/\"+i+\"/\"+str(count)+\".txt\"   #ชื่อใหม่\n",
        "    os.rename(oldPathOfFileInfolder,newPathOfFileInfolder)  #เปลี่ยนชื่อโดยเรียงลำดับจาก revision ที่ทำก่อนไปถึงล่าสุด (1...n)\n",
        " \n",
        "    # f = open(newPathOfFileInfolder, \"r\")\n",
        "    # clean_msg(f.read(),i+\"/\"+str(count)+\".txt\") #ส่งไฟล์ที่ rename ตะกี้ไป clean (args ขวาคือส่งตำแหน่งใหม่/ชื่อไฟล์ใหม่ไปด้วยเพราะจะ overwrite ลงไฟล์ที่ rename นั่นแหละ)\n",
        "\n",
        "    nameNewClean = newPathOfFileInfolder[0:-4] + \"_clean.txt\"\n",
        "    f1 = open(newPathOfFileInfolder, \"r\")\n",
        "    clean_msg(f1.read(),nameNewClean)\n",
        "    count+=1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jvg0suN-iPSg"
      },
      "source": [
        "pathToClean = \"/content/gdrive/My Drive/TOTAL/‘ขอโทษจากใจ ไม่ใช่ความผิดพวกคุณ’ เปิดจดหมายเลย์ออฟพนักงาน 1,900 ชีวิตจากซีอีโอ Airbnb  proofread oui/2.txt\"\n",
        "nameNewClean = pathToClean[0:-4] + \"_clean.txt\"\n",
        "f1 = open(pathToClean, \"r\")\n",
        "\n",
        "clean_msg(f1.read(),nameNewClean)\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jKhfp306LKHG"
      },
      "source": [
        "import re\n",
        "import string\n",
        "def clean_msg(msg,pathNewFileClean):\n",
        "    \n",
        "    # ลบ text ที่อยู่ในวงเล็บ <> ทั้งหมด\n",
        "    msg = re.sub(r'<.*?>','', msg)\n",
        " \n",
        "    #ลบ Link url\n",
        "    msg = re.sub(r'http\\S+', '', msg)\n",
        "    \n",
        "    # ลบ hashtag\n",
        "    msg = re.sub(r'#','',msg)\n",
        "    \n",
        "    # ลบ เครื่องหมายคำพูด (punctuation)\n",
        "    for c in string.punctuation:\n",
        "      if c!=\".\":\n",
        "        msg = re.sub(r'\\{}'.format(c),'',msg)\n",
        "    #ส่วนเปลี่ยน whitespace ที่มากกว่า 1 ให้เหลือ whitespace แค่ 1\n",
        "    #msg = msg.replace(\"  \",\" \")\n",
        "    msg = re.sub(' +',' ',msg)\n",
        " \n",
        "    lines = msg.split(\"\\n\")\n",
        "    non_empty_lines = [line for line in lines if line.strip() != \"\"]\n",
        "  \n",
        "    string_without_empty_lines = \"\"\n",
        "    for line in non_empty_lines:\n",
        "          string_without_empty_lines += line + \"\\n\"\n",
        "  \n",
        "    msg = string_without_empty_lines\n",
        "\n",
        "    msg = msg.replace(\"\\ufeff\",\"\")\n",
        "    # msg = msg.replace(\"\\ufeff\\n\",\"\")\n",
        "    \n",
        "    if msg[0:2]==\" \\n\":\n",
        "      msg = msg[2:]\n",
        "    elif msg[0:1]==\"\\n\":\n",
        "      msg = msg[1:]\n",
        "    f = open(pathNewFileClean, \"w\")\n",
        "    f.write(msg)\n",
        "    f.close()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q0ocf1VO4Kjl"
      },
      "source": [
        "import os\n",
        "\n",
        "location = \"/content/gdrive/My Drive/TOTAL\"\n",
        "listFolderInPath = os.listdir(location) #list ของ Folder ที่อยู่ใน Location \n",
        "for i in listFolderInPath:\n",
        "  eachPathFolder = location + \"/\" + i\n",
        "  listOfFileInFolder = os.listdir(eachPathFolder) #list ของชื่อไฟล์ในโฟลเดอร์นั้น\n",
        "  \n",
        "  for j in listOfFileInFolder:\n",
        "    oldPathOfFileInfolder = \"/content/gdrive/My Drive/TOTAL/\"+i+\"/\"+j\n",
        "\n",
        "    if j[-10:]==\"_clean.txt\":\n",
        "      # print(oldPathOfFileInfolder)\n",
        "      !time attacut-cli \"{oldPathOfFileInfolder}\" \\--model=attacut-sc \\--gpu \\--batch-size=100 \\--num-cores=1\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kkQoyqpT8KTL"
      },
      "source": [
        "Diff word"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1kawf0nY2rGW"
      },
      "source": [
        "Find Word"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gjOUs6mHmbL6"
      },
      "source": [
        "from attacut import tokenize, Tokenizer\n",
        "def findWord(start,end,listWordNormal,listWordClean,path):\n",
        "\n",
        "\n",
        "\n",
        "  text_1 = open(path).read()\n",
        "  runReverseBefore = start\n",
        "  countBefore = 0\n",
        "  while (runReverseBefore >= 0):\n",
        "    if (text_1[runReverseBefore] == \" \" or text_1[runReverseBefore] == \"\\n\" or text_1[runReverseBefore] == \"\\t\"):\n",
        "      countBefore += 1\n",
        "    runReverseBefore -= 1\n",
        "\n",
        "  countBetweenEndToStart = 0\n",
        "  runBetweenEndToStart = end-1\n",
        "  while (runBetweenEndToStart>start):\n",
        "    if (text_1[runBetweenEndToStart] == \" \" or text_1[runBetweenEndToStart] == \"\\n\" or text_1[runBetweenEndToStart] == \"\\t\"):\n",
        "      countBetweenEndToStart += 1\n",
        "    runBetweenEndToStart -= 1\n",
        "  # print(\"เจอแปลกๆก่อนหน้า :\",countBefore,\"ตัว\")\n",
        "  # print(\"เจอแปลกๆระหว่าง start และ end :\",countBetweenEndToStart,\"ตัว\")\n",
        "\n",
        "  \n",
        "\n",
        "\n",
        "  # runWord_old=0\n",
        "  # runCharacter_old=0\n",
        "  # keep_runWord_old = []\n",
        "  # for eachWord1 in listWordNormal:\n",
        "  #   for eachCha1 in eachWord1:\n",
        "  #     if (runCharacter_old in range(start,end)):\n",
        "  #       if (runCharacter_old == start):\n",
        "  #         # print(\"[\"+listWordNormal[runWord_old]+\"]\",\"list of x\")\n",
        "  #         print(eachCha1,\"x1\")\n",
        "  #       if (runCharacter_old == end-1):\n",
        "  #         print(eachCha1,\"x2\")\n",
        "  #       keep_runWord_old.append(runWord_old)\n",
        "  #     runCharacter_old += 1 \n",
        "  #   runWord_old += 1\n",
        "  # keep_runWord_old = list(dict.fromkeys(keep_runWord_old))\n",
        "  \n",
        "\n",
        "  new_start = start - countBefore\n",
        "  new_end = end - countBefore - countBetweenEndToStart\n",
        "\n",
        "\n",
        "  runWord_new=0\n",
        "  runCharacter_new=0\n",
        "  keep_runWord_new = []\n",
        "  for eachWord2 in listWordClean:\n",
        "    for eachCha2 in eachWord2:\n",
        "      if (runCharacter_new in range(new_start,new_end)):\n",
        "        # if (runCharacter_new == new_start):\n",
        "        #   # print(\"[\"+listWordClean[runWord_new]+\"]\",\"list of y\")\n",
        "        #   print(eachCha2,\"y1\")\n",
        "        # if (runCharacter_new == new_end-1):\n",
        "        #   print(eachCha2,\"y2\\n\")\n",
        "        keep_runWord_new.append(runWord_new)\n",
        "      runCharacter_new += 1 \n",
        "    runWord_new += 1\n",
        "  keep_runWord_new = list(dict.fromkeys(keep_runWord_new))\n",
        "  # print(keep_runWord_new)\n",
        "  word=\"\"\n",
        "  for i in keep_runWord_new:\n",
        "    word = word + listWordClean[i] + \" \"\n",
        "  \n",
        "  newRange = []\n",
        "  newRange.append(new_start)\n",
        "  newRange.append(new_end)\n",
        "\n",
        "  return word , newRange;"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AeSzyUsIJMco"
      },
      "source": [
        "เก็บ Stat"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8h88UI20JMEZ"
      },
      "source": [
        "def stat(beforeReplace,afterReplace,beforeWord,afterWord):\n",
        "  list_misSpell = []\n",
        "  list_contraction = []\n",
        "  list_mostChange = []\n",
        "\n",
        "\n",
        "  count_misSpell = 0 \n",
        "  count_misSpell = len(beforeReplace)\n",
        "  # for key in beforeReplace:\n",
        "  #   list_misSpell.append(beforeReplace[key])\n",
        "  for i in beforeWord:\n",
        "    list_misSpell.append(i)\n",
        "\n",
        "\n",
        "\n",
        "  count_mostChange = 0\n",
        "  for (k1,v1), (k2,v2) in zip(beforeReplace.items(),afterReplace.items()):\n",
        "    list_temp1 = []\n",
        "    if (abs(len(v1)-len(v2)) > 20):\n",
        "      list_temp1.append(v1)\n",
        "      list_temp1.append(v2)\n",
        "      list_mostChange.append(list_temp1)\n",
        "      count_mostChange += 1\n",
        "\n",
        "  count_contraction = 0\n",
        "  # print(beforeWord)\n",
        "  # print(afterWord)\n",
        "  \n",
        "  for v1, v2 in zip(beforeWord,afterWord):\n",
        "    list_temp2 = [] \n",
        "    v1=v1.replace(\" \",\"\")\n",
        "    v2=v2.replace(\" \",\"\")\n",
        "    \n",
        "    count_dot1 = 0\n",
        "    for i in v1:\n",
        "      if (i == \".\"):\n",
        "        count_dot1 += 1\n",
        "    count_dot2 = 0\n",
        "    for j in v2:\n",
        "      if (j == \".\"):\n",
        "        count_dot2 += 1\n",
        "    # print(count_dot1)\n",
        "    # print(count_dot2)\n",
        "    if (count_dot1 > 1 or count_dot2 > 1):\n",
        "      if (len(v1)==2*count_dot1 or len(v2)==2*count_dot2):\n",
        "        list_temp2.append(v1)\n",
        "        list_temp2.append(v2)\n",
        "        list_contraction.append(list_temp2)\n",
        "        count_contraction +=1\n",
        "  print(\"จำนวนคำที่เปลี่ยนแปลงจากตัวย่อเป็นตัวเต็ม :\",count_contraction)\n",
        "  if (len(list_contraction)!=0):\n",
        "    print(list_contraction,\"\\n\")\n",
        "  else:\n",
        "    print(\"\\n\")\n",
        "  print(\"จำนวนคำผิด :\",count_misSpell)\n",
        "  if (len(list_misSpell)!=0):\n",
        "    print(list_misSpell,\"\\n\")\n",
        "  print(\"จำนวนคำที่เปลี่ยนแปลงเยอะ :\",count_mostChange)\n",
        "  if (len(list_mostChange)!=0):\n",
        "    print(list_mostChange,\"\\n\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C5hqSJtH2vtQ"
      },
      "source": [
        "Find Sentence"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ANoyg8nrkFfh"
      },
      "source": [
        "from pythainlp.tokenize import sent_tokenize\n",
        "from attacut import tokenize, Tokenizer\n",
        "\n",
        "def findSentence(range_sentence,list_sentence):\n",
        "  start = range_sentence[0]\n",
        "  end = range_sentence[1]\n",
        "\n",
        "  count_sentence = 0\n",
        "  count_character = 0\n",
        "  keep_sentence = []\n",
        "  for sentence in list_sentence:\n",
        "    for character in sentence:\n",
        "      if count_character in range(start,end):\n",
        "        keep_sentence.append(count_sentence)\n",
        "      count_character += 1\n",
        "    count_sentence += 1\n",
        "  keep_sentence = list(dict.fromkeys(keep_sentence))\n",
        "\n",
        "  text=\"\"\n",
        "  for i in keep_sentence:\n",
        "    text += list_sentence[i] + \" \"\n",
        "\n",
        "\n",
        "  if (end-start <= 11 and len(keep_sentence) !=0):\n",
        "    if (min(keep_sentence)-1 >=0 and max(keep_sentence)+1 <=len(list_sentence)-1):\n",
        "      text = list_sentence[min(keep_sentence)-1]+ \" \"+ text + \" \" +list_sentence[max(keep_sentence)+1]\n",
        "  return text\n",
        "\n",
        "    "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j6YJa-BMBGeH"
      },
      "source": [
        "def inline_diff(a, b):\n",
        "    matcher = difflib.SequenceMatcher('', a, b)\n",
        "    def process_tag(tag, i1, i2, j1, j2):\n",
        "        if tag == 'replace':\n",
        "            beforeReplace[(i1,i2)] = matcher.a[i1:i2]\n",
        "            afterReplace[(j1,j2)] = matcher.b[j1:j2]\n",
        "            return '{' + matcher.a[i1:i2] + ' -> ' + matcher.b[j1:j2] +'}' + \" ++++\"+'{:7}   a[{}:{}] --> b[{}:{}]'.format(tag, i1, i2, j1, j2, a[i1:i2], b[j1:j2])+ \"++++ \\n\"\n",
        "        if tag == 'delete':\n",
        "            dictDelete[(i1,i2)] = matcher.a[i1:i2]\n",
        "            return '{- ' + matcher.a[i1:i2]  +'}' + \" ++++\"+'{:7}   a[{}:{}] --> b[{}:{}]'.format(tag, i1, i2, j1, j2, a[i1:i2], b[j1:j2])+ \"++++ \\n\"\n",
        "        if tag == 'equal':\n",
        "            return matcher.a[i1:i2]\n",
        "        if tag == 'insert':\n",
        "            dictInsert[(j1,j2)] = matcher.b[j1:j2]\n",
        "            return '{+ ' + matcher.b[j1:j2]  +'}'+\" ++++\"+'{:7}   a[{}:{}] --> b[{}:{}]'.format(tag, i1, i2, j1, j2, a[i1:i2], b[j1:j2])+ \"++++ \\n\"\n",
        "        assert False, \"Unknown tag %r\"%tag\n",
        "    return ''.join(process_tag(*t) for t in matcher.get_opcodes())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C84Q48TpKW_Z"
      },
      "source": [
        "import difflib\n",
        "from attacut import tokenize, Tokenizer\n",
        "sentence_1=[]\n",
        "sentence_2=[]\n",
        "atta = Tokenizer(model=\"attacut-sc\")\n",
        "beforeReplace = {}\n",
        "afterReplace = {}\n",
        "dictDelete = {}\n",
        "dictInsert = {}\n",
        "\n",
        "\n",
        "path1toDiff = \"/content/gdrive/MyDrive/TOTAL/เปิดเส้นทาง ‘หมู่อาร์ม’ กับการเปิดเผยข้อมูลทุจริตเบี้ยเลี้ยงทหาร  proofread pa/1_clean.txt\"\n",
        "path2toDiff = \"/content/gdrive/MyDrive/TOTAL/เปิดเส้นทาง ‘หมู่อาร์ม’ กับการเปิดเผยข้อมูลทุจริตเบี้ยเลี้ยงทหาร  proofread pa/2_clean.txt\"\n",
        "\n",
        "# path1toDiff = \"/content/gdrive/MyDrive/TOTAL/EIC ปรับลดประมาณการเศรษฐกิจไทยปีนี้หดตัว 7.3% จาก~มติดลบ 5.6% คาดใช้เวลา 2 ปี GDP ฟื้นตัวเท่าปี 2562 Proofread Pat/2_clean.txt\"\n",
        "# path2toDiff = \"/content/gdrive/MyDrive/TOTAL/EIC ปรับลดประมาณการเศรษฐกิจไทยปีนี้หดตัว 7.3% จาก~มติดลบ 5.6% คาดใช้เวลา 2 ปี GDP ฟื้นตัวเท่าปี 2562 Proofread Pat/1_clean.txt\"\n",
        "print(inline_diff(open(path1toDiff).read(),open(path2toDiff).read()))\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "#FindWord\n",
        "beforeWord=[]\n",
        "afterWord=[]\n",
        "\n",
        "f_tokenize1 = open(path1toDiff,\"r\")\n",
        "list_before_word = atta.tokenize(f_tokenize1.read())\n",
        "list_before_word_clean = []\n",
        "for i in list_before_word:\n",
        "  if (i!=\" \" and i!=\"\\n\" and i!=\"\\t\"):\n",
        "    i = i.replace(\" \",\"\")\n",
        "    i = i.replace(\"\\n\",\"\")\n",
        "    i = i.replace(\"\\t\",\"\")\n",
        "    list_before_word_clean.append(i)\n",
        "\n",
        "f_tokenize2 = open(path2toDiff,\"r\")\n",
        "list_after_word = atta.tokenize(f_tokenize2.read())\n",
        "list_after_word_clean = []\n",
        "for i in list_after_word :\n",
        "  if (i!=\" \" and i!=\"\\n\" and i!=\"\\t\"):\n",
        "    i = i.replace(\" \",\"\")\n",
        "    i = i.replace(\"\\n\",\"\")\n",
        "    i = i.replace(\"\\t\",\"\")\n",
        "    list_after_word_clean.append(i)\n",
        "\n",
        "\n",
        "# print(beforeReplace)\n",
        "# print(afterReplace)\n",
        "\n",
        "# print(\"Before :\")\n",
        "# print(list_before_word)\n",
        "# print(list_before_word_clean)\n",
        "# print(list_sentence_Before)\n",
        "# print(\"After :\")\n",
        "# print(list_after_word)\n",
        "# print(list_after_word_clean)\n",
        "# print(list_sentence_After)\n",
        "\n",
        "\n",
        "list_sentence_Before = sent_tokenize(open(path1toDiff).read(), engine=\"whitespace+newline\")\n",
        "list_sentence_After = sent_tokenize(open(path2toDiff).read(), engine=\"whitespace+newline\")\n",
        "\n",
        "#restore key\n",
        "restorebeforeReplace = {}\n",
        "for key in beforeReplace:\n",
        "  runCha1 = 0\n",
        "  for i in beforeReplace[key]:\n",
        "    if (i==\" \"):\n",
        "      runCha1 += 1\n",
        "    if (i==\"\\t\"):\n",
        "      runCha1 += 1\n",
        "    if (i==\"\\n\"):\n",
        "      runCha1 += 1\n",
        "    else:\n",
        "      if (beforeReplace[key][-1] == \" \" or beforeReplace[key][-1] == \"\\n\" or beforeReplace[key][-1] == \"\\t\"):\n",
        "        restorebeforeReplace[(key[0]+runCha1,key[1]-1)] = beforeReplace[key]\n",
        "        break\n",
        "      else:\n",
        "        restorebeforeReplace[(key[0]+runCha1,key[1])] = beforeReplace[key]\n",
        "        break\n",
        "\n",
        "\n",
        "restoreafterReplace = {}\n",
        "for key in afterReplace:\n",
        "  runCha2 = 0\n",
        "  count_y=0\n",
        "  for i in afterReplace[key]:\n",
        "    if (i==\" \"):\n",
        "      runCha2 += 1\n",
        "    elif (i==\"\\t\"):\n",
        "      runCha2 += 1\n",
        "    elif (i==\"\\n\"):\n",
        "      runCha2 += 1\n",
        "    else:\n",
        "      if (afterReplace[key][-1] == \" \" or afterReplace[key][-1] == \"\\n\" or afterReplace[key][-1] == \"\\t\"):\n",
        "        restoreafterReplace[(key[0]+runCha2,key[1]-1)] = afterReplace[key]\n",
        "        break\n",
        "      else:\n",
        "        restoreafterReplace[(key[0]+runCha2,key[1])] = afterReplace[key]\n",
        "        break\n",
        "    \n",
        "for key in restorebeforeReplace:\n",
        "  wBefore ,newRange_Before = findWord(key[0],key[1],list_before_word,list_before_word_clean,path1toDiff)\n",
        "  beforeWord.append(wBefore)\n",
        "  sentence_1.append(findSentence(newRange_Before, list_sentence_Before))\n",
        "\n",
        "for key in restoreafterReplace:\n",
        "  wAfter ,newRange_After = findWord(key[0],key[1],list_after_word,list_after_word_clean,path2toDiff)\n",
        "  afterWord.append(wAfter)\n",
        "  sentence_2.append(findSentence(newRange_After, list_sentence_After))\n",
        "\n",
        "print(\"///////////////////////////////////////////////////////////////////////\\n\")\n",
        "\n",
        "# print(len(beforeWord))\n",
        "# print(len(afterWord))\n",
        "# print(len(beforeWord))\n",
        "# print(len(afterWord))\n",
        "count_answer = 0\n",
        "for (k,v), (k2,v2) in zip(restorebeforeReplace.items(),restoreafterReplace.items()):\n",
        "  print(\"<<<CHANGE>>>\\t\",'\"'+v+'\"',\"to\",'\"'+v2+'\"')\n",
        "  print(\"<<<WORD>>>\\t\",'\"'+beforeWord[count_answer]+'\"',\"to\",'\"'+afterWord[count_answer]+'\"')\n",
        "  print(\"<<<SENTENCE>>>\\t\",'\"'+sentence_1[count_answer]+'\"',\"to\",'\"'+sentence_2[count_answer]+'\"',\"\\n\")\n",
        "  count_answer += 1\n",
        "print(\"///////////////////////////////////////////////////////////////////////\\n\")\n",
        "# print(sentence_1)\n",
        "# print(sentence_2,\"\\n\")\n",
        "\n",
        "stat(beforeReplace,afterReplace,beforeWord,afterWord)\n",
        "\n",
        "print(\"จำนวนคำที่ลบ :\",len(dictDelete))\n",
        "print(dictDelete,\"\\n\")\n",
        "print(\"จำนวนคำที่เพิ่ม :\",len(dictInsert))\n",
        "if (len(dictInsert)!=0):\n",
        "  print(dictInsert,\"\\n\")\n",
        "else:\n",
        "  print(\"\\n\")\n",
        "print(\"จำนวนประโยคต่อข่าวล่าสุด :\",len(list_sentence_After))\n",
        "print(list_sentence_After,\"\\n\")\n",
        "\n",
        "dict_cha_per_sentence = {}\n",
        "for sentence in list_sentence_After:\n",
        "  count_cha_per_sentence = 0\n",
        "  for character in sentence:\n",
        "    count_cha_per_sentence +=1\n",
        "  dict_cha_per_sentence[sentence] = count_cha_per_sentence\n",
        "\n",
        "print(\"จำนวนอักขระต่อประโยค :\")\n",
        "print(dict_cha_per_sentence,\"\\n\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ow24yWWa1L2L"
      },
      "source": [
        "แบบ 1"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Fk1msjsd1ItH"
      },
      "source": [
        "import csv\n",
        "import os\n",
        "\n",
        "\n",
        "location = \"/content/gdrive/My Drive/TOTAL\"\n",
        "listFolderInPath = os.listdir(location) #list ของ Folder ที่อยู่ใน Location \n",
        "count = 1\n",
        "with open('table_news.csv', 'w') as f1:\n",
        "  b1 = csv.DictWriter(f1,[\"news_id\", \"news_name\"])\n",
        "  b1.writeheader()\n",
        "  for i in listFolderInPath:\n",
        "    if (i!=\".ipynb_checkpoints\"):\n",
        "      put = [{\"news_id\":count,\"news_name\":i}]\n",
        "      b1.writerows(put)\n",
        "      count += 1\n",
        "\n",
        "\n",
        "with open('stat1.csv', 'w') as f2:\n",
        "  # writer.writerow([\"news_id\", \"rev_before\", \"rev_after\", \"edit_pos_before\", \"words_before\", \"edit_pos_after\", \"words_after\",\"sentence_before\",\"sentence_after\"])\n",
        "  writerr = csv.DictWriter(f2,[\"news_id\", \"rev_before\", \"rev_after\", \"edit_pos_before\", \"before\", \"words_before\",\"sentence_before\", \"edit_pos_after\", \"after\", \"words_after\",\"sentence_after\"])\n",
        "  writerr.writeheader()\n",
        "  count_news = 1\n",
        "  for i in listFolderInPath:\n",
        "    temp = []\n",
        "    eachPathFolder = location + \"/\" + i\n",
        "    listOfFileInFolder = os.listdir(eachPathFolder) #list ของชื่อไฟล์ในโฟลเดอร์นั้น\n",
        "    if (i!=\".ipynb_checkpoints\"):\n",
        "      for j in listOfFileInFolder:\n",
        "        oldPathOfFileInfolder = \"/content/gdrive/My Drive/TOTAL/\"+i+\"/\"+j\n",
        "        if j[-10:]==\"_clean.txt\":\n",
        "          temp.append(j)\n",
        "      # print(temp)\n",
        "      keep2 = []\n",
        "      for k in range(len(temp)-1):\n",
        "        keep1 = []\n",
        "        keep1.append(k+1)\n",
        "        keep1.append(k+2)\n",
        "        keep2.append(keep1)\n",
        "      # keep2.reverse()\n",
        "      # print(keep2)\n",
        "      for l in keep2:\n",
        "        count_test = 0\n",
        "        beforeReplace = {}\n",
        "        afterReplace = {}\n",
        "        beforeReplace,afterReplace,beforeWord,afterWord,sentence_1,sentence_2 = difff(\"/content/gdrive/My Drive/TOTAL/\"+i+\"/\"+str(l[0])+\"_clean.txt\",\"/content/gdrive/My Drive/TOTAL/\"+i+\"/\"+str(l[1])+\"_clean.txt\")\n",
        "        # print(\"/////\",l[0],\"to\",l[1],\"/////\")\n",
        "        if (len(beforeReplace) == 0):\n",
        "          break\n",
        "        else :  \n",
        "          for (k,v), (k2,v2) in zip(beforeReplace.items(),afterReplace.items()):\n",
        "              khomun = [{'news_id': count_news,'rev_before': l[0],'rev_after': l[1],'edit_pos_before':k,\"before\":v,\"words_before\":beforeWord[count_test],\"sentence_before\":sentence_1[count_test], \"edit_pos_after\":k2,\"after\":v2, \"words_after\":afterWord[count_test],\"sentence_after\":sentence_2[count_test]}]\n",
        "              # writer.writerow([i, l[0], l[1], k, v, k2, v2,sentence_1[count_test],sentence_2[count_test]])\n",
        "              writerr.writerows(khomun)\n",
        "              count_test += 1\n",
        "    count_news += 1\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tVCFsLRM1KWY"
      },
      "source": [
        "แบบ 2"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N2sEqbtHyQwS"
      },
      "source": [
        "import csv\n",
        "import os\n",
        "\n",
        "\n",
        "location = \"/content/gdrive/My Drive/TOTAL\"\n",
        "listFolderInPath = os.listdir(location) #list ของ Folder ที่อยู่ใน Location \n",
        "\n",
        "with open('stat2.csv', 'w') as f:\n",
        "  # writer.writerow([\"news_id\", \"rev_before\", \"rev_after\", \"edit_pos_before\", \"words_before\", \"edit_pos_after\", \"words_after\",\"sentence_before\",\"sentence_after\"])\n",
        "  writerr = csv.DictWriter(f,[\"news_id\", \"rev_before\", \"rev_after\", \"edit_pos_before\", \"words_before\", \"edit_pos_after\", \"words_after\",\"sentence_before\",\"sentence_after\"])\n",
        "  writerr.writeheader()\n",
        "  for i in listFolderInPath:\n",
        "    print(\"**********************************\",i)\n",
        "    temp = []\n",
        "    run_news = 0\n",
        "    eachPathFolder = location + \"/\" + i\n",
        "    listOfFileInFolder = os.listdir(eachPathFolder) #list ของชื่อไฟล์ในโฟลเดอร์นั้น\n",
        "    if (i!=\".ipynb_checkpoints\"):\n",
        "      for j in listOfFileInFolder:\n",
        "        oldPathOfFileInfolder = \"/content/gdrive/My Drive/TOTAL/\"+i+\"/\"+j\n",
        "        if j[-10:]==\"_clean.txt\":\n",
        "          temp.append(j)\n",
        "      print(temp)\n",
        "      keep2 = []\n",
        "\n",
        "      for k in range(len(temp)-1):\n",
        "        keep1 = []\n",
        "        keep1.append(k+2)\n",
        "        keep1.append(k+1)\n",
        "        keep2.append(keep1)\n",
        "      keep2.reverse()\n",
        "\n",
        "      # print(keep2,\"\\n\")\n",
        "\n",
        "      for l in keep2:\n",
        "        count_rev = 0\n",
        "        count_test = 0\n",
        "        # print(\"/content/gdrive/My Drive/TOTAL/\"+i+\"/\"+str(l[0])+\".txt\")\n",
        "        # print(\"/content/gdrive/My Drive/TOTAL/\"+i+\"/\"+str(l[1])+\".txt\")\n",
        "        beforeReplace = {}\n",
        "        afterReplace = {}\n",
        "        beforeReplace,afterReplace,beforeWord,afterWord,sentence_1,sentence_2 = difff(\"/content/gdrive/My Drive/TOTAL/\"+i+\"/\"+str(l[0])+\"_clean.txt\",\"/content/gdrive/My Drive/TOTAL/\"+i+\"/\"+str(l[1])+\"_clean.txt\")\n",
        "        print(\"/////\",l[0],\"to\",l[1],\"/////\")\n",
        "        if (len(beforeReplace) == 0):\n",
        "          print(\"case1\")\n",
        "          break\n",
        "        else :  \n",
        "          for (k,v), (k2,v2) in zip(beforeReplace.items(),afterReplace.items()):\n",
        "            if (run_news == 0 and count_rev == 0):\n",
        "              print(\"case2\")\n",
        "              khomun = [{'news_id': i,'rev_before': l[0],'rev_after': l[1],'edit_pos_before':k,\"words_before\":beforeWord[count_test], \"edit_pos_after\":k2, \"words_after\":afterWord[count_test],\"sentence_before\":sentence_1[count_test],\"sentence_after\":sentence_2[count_test]}]\n",
        "              # writer.writerow([i, l[0], l[1], k, v, k2, v2,sentence_1[count_test],sentence_2[count_test]])\n",
        "              writerr.writerows(khomun)\n",
        "              count_test += 1\n",
        "              run_news += 1\n",
        "              count_rev += 1\n",
        "            elif (count_rev == 0):\n",
        "              print(\"case3\")\n",
        "              khomun = [{'rev_before': l[0],'rev_after': l[1],'edit_pos_before':k,\"words_before\":beforeWord[count_test], \"edit_pos_after\":k2, \"words_after\":afterWord[count_test],\"sentence_before\":sentence_1[count_test],\"sentence_after\":sentence_2[count_test]}]\n",
        "              writerr.writerows(khomun)\n",
        "              count_test += 1\n",
        "              count_rev += 1\n",
        "            else:\n",
        "              print(\"case4\")\n",
        "              khomun = [{'edit_pos_before':k,\"words_before\":beforeWord[count_test], \"edit_pos_after\":k2, \"words_after\":afterWord[count_test],\"sentence_before\":sentence_1[count_test],\"sentence_after\":sentence_2[count_test]}]\n",
        "              writerr.writerows(khomun)\n",
        "              count_test += 1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HC1RgBl68wo-"
      },
      "source": [
        "import difflib\n",
        "from attacut import tokenize, Tokenizer\n",
        "\n",
        "def difff(pathBefore,pathAfter):\n",
        "  sentence_1=[]\n",
        "  sentence_2=[]\n",
        "  atta = Tokenizer(model=\"attacut-sc\")\n",
        "  dictDelete = {}\n",
        "  dictInsert = {}\n",
        "\n",
        "  inline_diff(open(pathBefore).read(),open(pathAfter).read())\n",
        "\n",
        "  #FindWord\n",
        "  beforeWord=[]\n",
        "  afterWord=[]\n",
        "\n",
        "  f_tokenize1 = open(pathBefore,\"r\")\n",
        "  list_before_word = atta.tokenize(f_tokenize1.read())\n",
        "  list_before_word_clean = []\n",
        "  for i in list_before_word:\n",
        "    if (i!=\" \" and i!=\"\\n\" and i!=\"\\t\"):\n",
        "      i = i.replace(\" \",\"\")\n",
        "      i = i.replace(\"\\n\",\"\")\n",
        "      i = i.replace(\"\\t\",\"\")\n",
        "      list_before_word_clean.append(i)\n",
        "\n",
        "  f_tokenize2 = open(pathAfter,\"r\")\n",
        "  list_after_word = atta.tokenize(f_tokenize2.read())\n",
        "  list_after_word_clean = []\n",
        "  for i in list_after_word :\n",
        "    if (i!=\" \" and i!=\"\\n\" and i!=\"\\t\"):\n",
        "      i = i.replace(\" \",\"\")\n",
        "      i = i.replace(\"\\n\",\"\")\n",
        "      i = i.replace(\"\\t\",\"\")\n",
        "      list_after_word_clean.append(i)\n",
        "\n",
        "  list_sentence_Before = sent_tokenize(open(pathBefore).read(), engine=\"whitespace+newline\")\n",
        "  list_sentence_After = sent_tokenize(open(pathAfter).read(), engine=\"whitespace+newline\")\n",
        "\n",
        "  #restore key\n",
        "  restorebeforeReplace = {}\n",
        "  for key in beforeReplace:\n",
        "    runCha1 = 0\n",
        "    for i in beforeReplace[key]:\n",
        "      if (i==\" \"):\n",
        "        runCha1 += 1\n",
        "      if (i==\"\\t\"):\n",
        "        runCha1 += 1\n",
        "      if (i==\"\\n\"):\n",
        "        runCha1 += 1\n",
        "      else:\n",
        "        if (beforeReplace[key][-1] == \" \" or beforeReplace[key][-1] == \"\\n\" or beforeReplace[key][-1] == \"\\t\"):\n",
        "          restorebeforeReplace[(key[0]+runCha1,key[1]-1)] = beforeReplace[key]\n",
        "          break\n",
        "        else:\n",
        "          restorebeforeReplace[(key[0]+runCha1,key[1])] = beforeReplace[key]\n",
        "          break\n",
        "\n",
        "\n",
        "  restoreafterReplace = {}\n",
        "  for key in afterReplace:\n",
        "    runCha2 = 0\n",
        "    count_y=0\n",
        "    for i in afterReplace[key]:\n",
        "      if (i==\" \"):\n",
        "        runCha2 += 1\n",
        "      elif (i==\"\\t\"):\n",
        "        runCha2 += 1\n",
        "      elif (i==\"\\n\"):\n",
        "        runCha2 += 1\n",
        "      else:\n",
        "        if (afterReplace[key][-1] == \" \" or afterReplace[key][-1] == \"\\n\" or afterReplace[key][-1] == \"\\t\"):\n",
        "          restoreafterReplace[(key[0]+runCha2,key[1]-1)] = afterReplace[key]\n",
        "          break\n",
        "        else:\n",
        "          restoreafterReplace[(key[0]+runCha2,key[1])] = afterReplace[key]\n",
        "          break\n",
        "      \n",
        "  for key in restorebeforeReplace:\n",
        "    wBefore ,newRange_Before = findWord(key[0],key[1],list_before_word,list_before_word_clean,pathBefore)\n",
        "    beforeWord.append(wBefore)\n",
        "    sentence_1.append(findSentence(newRange_Before, list_sentence_Before))\n",
        "\n",
        "  for key in restoreafterReplace:\n",
        "    wAfter ,newRange_After = findWord(key[0],key[1],list_after_word,list_after_word_clean,pathAfter)\n",
        "    afterWord.append(wAfter)\n",
        "    sentence_2.append(findSentence(newRange_After, list_sentence_After))\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "  return restorebeforeReplace,restoreafterReplace,beforeWord,afterWord,sentence_1,sentence_2\n",
        "\n",
        "  "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "s7IMnkyA97cM",
        "outputId": "324e6a5d-105d-4b09-d374-3ddf6719ce41"
      },
      "source": [
        "import csv\n",
        "import pandas as pd\n",
        "from langdetect import detect\n",
        "\n",
        "alphabet_stringUpper = string.ascii_uppercase\n",
        "alphabet_stringLower = string.ascii_lowercase\n",
        "alphabet_listUpper = list(alphabet_stringUpper)\n",
        "alphabet_listLower = list(alphabet_stringLower)\n",
        "# print(alphabet_listUpper)\n",
        "# print(alphabet_listLower)\n",
        "\n",
        "dict_tempLowerToUpper = {}\n",
        "dict_tempUpperToLower = {}\n",
        "\n",
        "for lower,upper in zip(alphabet_listLower,alphabet_listUpper):\n",
        "  dict_tempLowerToUpper[lower] = upper\n",
        "  dict_tempUpperToLower[upper] = lower\n",
        "\n",
        "print(dict_tempLowerToUpper)\n",
        "print(dict_tempUpperToLower)\n",
        "\n",
        "\n",
        "with open('stat1.csv','r') as csvinput:\n",
        "    with open('peace.csv', 'w') as csvoutput:\n",
        "        writer = csv.writer(csvoutput, lineterminator='\\n')\n",
        "        reader = csv.reader(csvinput)\n",
        "\n",
        "        all = []\n",
        "        row = next(reader)\n",
        "        row.append('Label')\n",
        "        all.append(row)\n",
        "        count = 0\n",
        "        for row in reader:\n",
        "          string_before1 = row[4]\n",
        "          string_after2 = row[8]\n",
        "          word_before1 = row[5]\n",
        "          word_after2 = row[9]\n",
        "          string_before = string_before1.replace(\" \",\"\")\n",
        "          string_after = string_after2.replace(\" \",\"\")\n",
        "          word_before = word_before1.replace(\" \",\"\")\n",
        "          word_after = word_after2.replace(\" \",\"\")\n",
        "          count_diff = sum(a!=b for a, b in zip(word_before, word_after))\n",
        "          count_dotBefore = 0\n",
        "          count_dotAfter = 0\n",
        "          for i in word_before:\n",
        "            if (i == \".\"):\n",
        "              count_dotBefore +=1\n",
        "          for j in word_after:\n",
        "            if (j == \".\"):\n",
        "              count_dotAfter +=1\n",
        "\n",
        "          if (string_before.lower() ==  string_after.lower()):\n",
        "            if (isEnglish(string_before)):\n",
        "              row.append(\"แก้ไขตัวเล็กตัวใหญ่ (English)\")\n",
        "              # print(\"case 1\",string_before,string_after)\n",
        "              count += 1\n",
        "            \n",
        "            elif (not isEnglish(string_before)):\n",
        "              row.append(\"คำเดิมแต่เพิ่มช่องว่าง\")\n",
        "              # print(\"case 2\",string_before,string_after)\n",
        "              count += 1\n",
        "          elif (abs(len(string_before1)-len(string_after2)) >= 10):\n",
        "            row.append(\"แก้ไขปริมาณมาก\")\n",
        "            # print(\"case 8\",string_before1,string_after2)\n",
        "            count += 1\n",
        "          \n",
        "          elif (count_diff  == 1 or count_diff  == 2): #ต่างกัน 1-2 ตัว\n",
        "            if (not isNumber(word_before) and not isNumber(word_after)): #ไม่ใช่ตัวเลข\n",
        "              if (len(word_before) == len(word_after)): #ขนาดต้องเท่าเดิม\n",
        "                if (countt(word_before, word_after) != 0):\n",
        "                  row.append(\"สะกดผิด\")\n",
        "                  # print(\"case 4\",word_before,word_after)\n",
        "                  count += 1\n",
        "                else:\n",
        "                  row.append(\"แก้ไขเครื่องหมายสนทนา\")\n",
        "                  # print(\"*2*\",word_before,word_after)\n",
        "                  count += 1\n",
        "              else:\n",
        "                row.append(\"แก้ไขปริมาณมาก\")\n",
        "                # print(\"*3*\",word_before,word_after)\n",
        "                count += 1\n",
        "            elif (isNumber(word_before) and isNumber(word_after)):\n",
        "              row.append(\"แก้ไขค่าตัวเลข\")\n",
        "              # print(\"*3*\",word_before,word_after)\n",
        "              count += 1\n",
        "            else:\n",
        "              row.append(\"แก้ไขคำ\")\n",
        "              # print(\"*3*\",word_before,word_after)\n",
        "              count += 1\n",
        "          elif (not isEnglish(word_before) and not isEnglish(word_after)):\n",
        "            if (word_before[0]!=\".\" and word_after[0]!=\".\"):\n",
        "              if (count_dotBefore >= 1 and count_dotAfter == 0):\n",
        "                if (countt(word_before.replace(\".\",\"\"), word_after) == len(word_before.replace(\".\",\"\"))):\n",
        "                  row.append(\"แก้ไขตัวย่อตัวเต็ม\")\n",
        "                  # print(count)\n",
        "                  # print(\"case 9\",word_before,word_after)\n",
        "                  count += 1\n",
        "                else:\n",
        "                  row.append(\"แก้ไขคำ\")\n",
        "                  # print(\"*5*\",word_before,word_after)\n",
        "                  count += 1\n",
        "              elif (count_dotBefore == 0 and count_dotAfter >= 1):\n",
        "                if (countt(word_after.replace(\".\",\"\"), word_before) == len(word_after.replace(\".\",\"\"))):\n",
        "                  row.append(\"แก้ไขตัวย่อตัวเต็ม\")\n",
        "                  # print(\"case 11\",word_before,word_after)\n",
        "                  count += 1\n",
        "                else:\n",
        "                  row.append(\"แก้ไขตัวย่อตัวเต็ม\")\n",
        "                  # print(\"*6*\",word_before,word_after)\n",
        "                  count += 1\n",
        "              else:\n",
        "                row.append(\"แก้ไขคำ\")\n",
        "                # print(\"*7*\",word_before,word_after)\n",
        "                count += 1\n",
        "            else:\n",
        "              row.append(\"แก้ไขคำ\")\n",
        "              # print(\"*8*\",word_before,word_after)\n",
        "              count += 1\n",
        "          \n",
        "          \n",
        "          \n",
        "          elif (len(word_before.replace(\" \",\"\")) == 0):\n",
        "            row.append(\"เติมคำจากช่องว่าง\")\n",
        "            # print(\"case 15\",word_before,word_after)\n",
        "            count += 1\n",
        "          elif (len(word_before) - len(word_after) <= 10):\n",
        "            row.append(\"แก้ไขคำ\")\n",
        "            # print(\"case 16\",word_before,word_after)\n",
        "            count += 1\n",
        "          else:\n",
        "            row.append(\"แก้ไขคำ\")\n",
        "            print(\"*9*\",word_before,word_after)\n",
        "            count += 1\n",
        "          all.append(row)\n",
        "        print(count)\n",
        "        writer.writerows(all)\n",
        "\n",
        "\n",
        "# Facebook เฟซบุ๊ก\n",
        "# Google กูเกิล\n",
        "# Podcast พอดแคสต์\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "{'a': 'A', 'b': 'B', 'c': 'C', 'd': 'D', 'e': 'E', 'f': 'F', 'g': 'G', 'h': 'H', 'i': 'I', 'j': 'J', 'k': 'K', 'l': 'L', 'm': 'M', 'n': 'N', 'o': 'O', 'p': 'P', 'q': 'Q', 'r': 'R', 's': 'S', 't': 'T', 'u': 'U', 'v': 'V', 'w': 'W', 'x': 'X', 'y': 'Y', 'z': 'Z'}\n",
            "{'A': 'a', 'B': 'b', 'C': 'c', 'D': 'd', 'E': 'e', 'F': 'f', 'G': 'g', 'H': 'h', 'I': 'i', 'J': 'j', 'K': 'k', 'L': 'l', 'M': 'm', 'N': 'n', 'O': 'o', 'P': 'p', 'Q': 'q', 'R': 'r', 'S': 's', 'T': 't', 'U': 'u', 'V': 'v', 'W': 'w', 'X': 'x', 'Y': 'y', 'Z': 'z'}\n",
            "*9* NewsEnvironmentPolitical Politics\n",
            "*9* ชื่อคลิปฟุตc0020c0021 31.3032\n",
            "*9* UPDATEminsreadby .Read\n",
            "*9* KEIRINGฟรองซัวส์ปิโนลต์ Kering\n",
            "*9* www.verblick.orgcovid19 Real\n",
            "*9* Prescriptiveanalytics ’เรือด่วน\n",
            "*9* NewsPoliticalPHOTOALBUM Politics\n",
            "*9* การเอาเด็กเป็นศูนย์กลาง childcentric\n",
            "*9* ‘คลาวด์ฟันดิงCloud tablet\n",
            "*9* เงินทุนจากรัฐบาลGovernment interaction\n",
            "*9* การเอาเด็กเป็นศูนย์กลาง childcentric\n",
            "*9* ‘คลาวด์ฟันดิงCloud tablet\n",
            "*9* เงินทุนจากรัฐบาลGovernment interaction\n",
            "*9* เงินทุนจากรัฐบาลGovernment funding\n",
            "*9* ดิตดร.รยุศด์บุญทัน Mins\n",
            "*9* จะเปิดให้เข้าเยี่ยมชมเมืองฮอว์กินส์จำลองแห่งนี้ตั้งแต่เดือนตุลาคมนี้เป็นต้นไปและสามารถศึกษาข้อมูลเพิ่มเติมได้ที่หรือ seriesandmaybeevenwitnesssomeunforeseenandunusualoccurrences.Signupnowtounlockthesecretsthatawait...\n",
            "*9* ภาพเฟซบุ๊กธวัชชัยไทย Photo\n",
            "*9* WEALTHNEWSTECHBUSINESS Tech\n",
            "*9* Contactessminsreadby Read\n",
            "*9* ค่ายจีเอ็มเอ็มแกรมมี่ GMMGrammy\n",
            "*9* ยอดจำหน่ายหูฟังไร้สายในปี millionunitsin\n",
            "*9* THAILANDTECHADVERTORIALmins และBig\n",
            "*9* พิสูจน์อักษรพรนภัสชำนาญค้า highlights\n",
            "*9* แฟลกชิปสโตร์รวมทั้งสิ้น SiamCenter\n",
            "16334\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-DLWOpUIh4Bk"
      },
      "source": [
        "># -*- coding: utf-8 -*-\n",
        "def isEnglish(s):\n",
        "    try:\n",
        "        s.encode(encoding='utf-8').decode('ascii')\n",
        "    except UnicodeDecodeError:\n",
        "        return False\n",
        "    else:\n",
        "        return True"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pTF9RgRv2jb0",
        "outputId": "13127693-a501-4842-aca5-868148874ecb"
      },
      "source": [
        "def countt(str1, str2):  \n",
        "    c, j = 0, 0\n",
        "\n",
        "    for i in str1:     \n",
        "\n",
        "        if str2.find(i)>= 0 and j == str1.find(i):  \n",
        "            c += 1\n",
        "        j += 1\n",
        "\n",
        "    return c\n",
        "# Main function \n",
        "str1 ='ม.ค.มี.ค.' # first string โรงพยาบาลสวรรค์ประชารักษ์ รพ.สวรรค์\n",
        "str2 ='มกราคมมีนาคม' # second string \n",
        "countt(str1, str2)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "3"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 393
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M9wTd1dHvJwG"
      },
      "source": [
        "count_diff = sum(a!=b for a, b in zip(\"นายแพทย์โจเซปท์สกาเลีย\", \"นายแพทย์โจเซฟสกาเลีย\"))\n",
        "for a, b in zip(\"นายแพทย์โจเซปท์สกาเลีย\", \"นายแพทย์โจเซฟสกาเลีย\"):\n",
        "  if(a!=b):\n",
        "    print(a,b)\n",
        "print(count_diff)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i1HmxN4YFcZ1"
      },
      "source": [
        "def isNumber(s): \n",
        "      \n",
        "    # handle for negative values \n",
        "    negative = False\n",
        "    if(s[0] =='-'): \n",
        "        negative = True\n",
        "          \n",
        "    if negative == True: \n",
        "        s = s[1:] \n",
        "      \n",
        "    # try to convert the string to int \n",
        "    try: \n",
        "        n = float(s) \n",
        "        return True\n",
        "    # catch exception if cannot be converted \n",
        "    except ValueError: \n",
        "        return False"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}